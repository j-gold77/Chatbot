René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.1Lecture 6Recommender SystemsPersonalization, Collaborative Filtering & Content-based recommendationCOMP 474/6741, Winter 2022René WitteDepartment of Computer Scienceand Software EngineeringConcordia UniversityRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.2Outline1 IntroductionModeling Users2 Collaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to other ItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluation3 Content-based RecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummary4 Notes and Further ReadingRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.3Slides CreditIncludes slides by Christopher D. Manning, Prabhakar Raghavan andHinrich Schütze [MRS08]• Copyright © 2008 Cambridge University PressRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.4Recommender Systems and Collaborative FilteringRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.5Collecting User InteractionsCopyright 2009 by Manning Publications Co., [Ala09]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.6Item MetadataCopyright 2009 by Manning Publications Co., [Ala09]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.7Netflix Recommendationshttps://www.youtube.com/watch?v=nq2QtatuF7Uhttps://www.youtube.com/watch?v=nq2QtatuF7URené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.81 Introduction2 Collaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to other ItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluation3 Content-based Recommendations4 Notes and Further ReadingRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.9Making RecommendationsGiven Information about a User. . .. . . we want to be able to have a system• recommending items (books, movies, music, photos, videos, etc.)• find users interested in a new item• find similar items, based on interests of other usersRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.10Collaborative FilteringCopyright 2016 by Manning Publications Co., [TB16]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.11Data CollectionCopyright 2016 by Manning Publications Co., [TB16]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.12Fun with Flags VectorsVectorsA vector ~v is an element of a vector space.• For example, ~v ∈ Rn with~v =x1x2...xn ∈ RnVisualizationWe can visualize vectors, e.g., in 2D:~vRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.13So what?Vectors of words, users, products, . . .We can represent (users, documents, products) as vectors, e.g., using the count oftags or the weight of words. This is called a vector space model.• Vector operations on entities, e.g., to compute their similarityCopyright 2008 by Cambridge University Press, [MRS08]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.14Movies as VectorsCopyright 2016 by Manning Publications Co., [TB16]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.15Length normalizationHow do we compute the length of a vector?• A vector can be (length-) normalized by dividing each of its components by itslength – here we use the L2 norm: ||x ||2 =√∑i x2i• This maps vectors onto the unit sphere . . .• . . . since after normalization: ||x ||2 =√∑i x2i = 1.0• As a result, longer and shorter vectors (more/fewer tags) have weights of thesame order of magnitude.→ Worksheet #5: Tasks 1, 2René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.16How do we formalize vector space similarity?Computing the similarity• First cut: (negative) distance between two points• ( = distance between the end points of the two vectors)• Euclidean distance?• Euclidean distance is a bad idea . . .• . . . because Euclidean distance is large for vectors of different lengths.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.17Why Euclidian distance is a bad idea0 101richpoorq: [rich poor]d1:Ranks of starving poets swelld2:Rich poor gap growsd3:Record baseball salaries in 2010The Euclidean distance of ~q and ~d2 is large although the distribution of terms in thequery q and the distribution of terms in the document d2 are very similar.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.18From angles to cosinesComparing vectors• The following two notions are equivalent.• Compare item vectors according to the angle between them, in decreasing order• Rank item vectors according to cosine(item1, item2) in increasing order• Cosine is a monotonically decreasing function of the angle for the interval[0◦,180◦]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.19CosineRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.20Cosine for normalized vectorsComputing similarity• For normalized vectors, the cosine is equivalent to the dot product or scalarproduct.• cos(~q, ~d) = ~q · ~d =∑i qi · di• (if ~q and ~d are length-normalized).→ Worksheet #5: Task 3René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.21Item RecommendationSimple Tag-based RecommendationCollaborative tagging gives rise to simple recommender approaches:• show other items (products, photos, videos, music) that were tagged similar byother users• exploited in many e-commerce/social networking web sitesRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.22Collaborative FilteringFinding related contentWhen multiple users tag the same resource, content can be discovered based onthe most frequent tags (example: Last.fm).René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.23RecommendationsRecommendations based on tagsWe can now exploit tags for a number of use cases:• Recommend items related to other items• Recommend items based on user’s interest• Find users interested in a new itemGeneral Approach• Represent users/items as(normalized) term vectors• Compute cosine similaritybetween vectors; i.e., the anglebetween them (for normalizedvectors, this is simply their dotproduct)Copyright 2008 by Cambridge University Press, [MRS08]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.24Items related to other itemsSimple point-to-point recommendation engine• Create item vectors using raw count• Normalize vectors• Compute cosine similarityResult is a similarity matrixRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.25Items of interest to a userPersonalization• Item-to-item is the same for all users• How can we recommend items for a particular user?Solution: build user-specific similarity matrix• computation of vectors, normalization as before• this time, we calculate the cosine similarity between a user vector andarticle vector→ Worksheet #5: Tasks 4, 5René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.26Finding relevant users for an itemRecommending items to users• New item comes in (blog post, photo, article, product, . . .)• Which users would be interested in it?Similar to before, compute similarity matrix between metadata of new item andmetadata of users.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.27Cold-Start ProblemGeneral issue in recommender system deployment• New user⇒ no user profile for recommendations• New item⇒ no user interactions for this itemNo general solution. . .Some strategies:• Ask user for preferences during sign-up• Recommend top-n items (e.g., currently most popular movies/songs/products)René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.28Semantic Vocabularies for User ModelingSemantic User ProfilesIdea: Use vocabularies instead of keywords in the vector representation of a userprofileMotivation• Semantic recommendations (remember the “tree” example)• Open knowledge bases:• interoperable between applications• controlled by users, not corporationsRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.29VocabulariesGeneric user modeling vocabulariesFOAF• The most popular generic user model offering descriptions for basic userinformation• No comprehensive classes for describing preferences or interestsGUMO• A generic user model that offers several classes for users’ characteristics• Basic user dimensions like Emotional States, Characteristics and PersonalityIntelLEO• Several ontologies strongly focused on personalization• Enables describing user and team modelling, preferences, tasks and interestsRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.30The $1m Netflix Prize Competition (2009)René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.31General machine learning processRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.32Performance EvaluationMeasuring performance• Is our fancy model better than giving out random recommendations?• We need metrics to evaluate and compare the performance of differentapproaches against a ground truth (a.k.a. gold standard)Precision and RecallThe precision provides a measure of the quality of the generated recommendations:precision =#correct system recommendations#all system recommendationsThe recall indicates how many relevant recommendations were found by a system:recall =#correct system recommendations#all correct recommendationsGenerally, there is a trade-off between precision and recall.→ Worksheet #5: Task 6René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.33Precision@kPrecision at cutoff k• Return a ranked list of recommendations (e.g., based on cosine similarity)• Evaluate only top-k recommendations (e.g., top-10)precision@k =1k·k∑c=1rel(c),where rel(c) tells us if item at rank c was relevant (1) or not (0).Intuitively. . .The percentage of correct recommendations in the top-k .Wait, what happened to Recall?Well. . . in this application scenario, we don’t really care (there are millions ofpotentially relevant items on Amazon or movies on Netflix)→ Worksheet #5: Task 7René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.34Average PrecisionAverage Precision at NIf we recommend N items to a user, where there are at most m relevant items in1 . . .N,AP@N =1mN∑k=1precision@k ·rel(k)again, rel(c) is 1 if the recommendation at rank c is relevant, 0 otherwiseNoteAP “rewards” (gives a higher score to) higher-ranked, correct recommendations→ Worksheet #5: Task 8René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.35Mean Average Precision (MAP)MAP• So far, everything was calculated for one user u ∈ U• But we want to know how well the system works across all users• Hence, average the AP for all users:MAP@N =1|U||U|∑u=1AP@N(u)But wait, there’s more. . .• Accuracy, Sensitivity, F-measure, . . .• Non-binary ranked results (i.e., not just correct or wrong, but a Likert-scale):Compute the discounted cumulative gain (DCG),DCGu = rel1 +|C|∑c=2relclog2 cRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.361 Introduction2 Collaborative Filtering3 Content-based RecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummary4 Notes and Further ReadingRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.37Content-based RecommendationsMotivation• So far, we build our model using vectors of concepts (e.g., tags, moviecategories, etc.)• What if we want to create recommendations based on the content• Movie description/summary• Blog post• News article• Research publication• . . .ApproachSame idea, but now we have to build vectors out of whole documents• Basic idea of information retrieval (IR)• Used in Internet search enginesRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.38Binary incidence matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 1 1 0 0 0 1BRUTUS 1 1 0 1 0 0CAESAR 1 1 0 1 1 1CALPURNIA 0 1 0 0 0 0CLEOPATRA 1 0 0 0 0 0MERCY 1 0 1 1 1 1WORSER 1 0 1 1 1 0. . .Each document is represented as a binary vector ∈ {0,1}|V |.[from Introduction to Information Retrieval]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.39Count matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 157 73 0 0 0 1BRUTUS 4 157 0 2 0 0CAESAR 232 227 0 2 1 0CALPURNIA 0 10 0 0 0 0CLEOPATRA 57 0 0 0 0 0MERCY 2 0 3 8 5 8WORSER 2 0 1 1 1 5. . .Each document is now represented as a count vector ∈ N|V |.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.40Bag of words model• We do not consider the order of words in a document.• John is quicker than Mary and Mary is quicker than John are represented thesame way.• This is called a bag of words model.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.41tf-idfTerm frequency tfThe term frequency tft,d of term t in document d is defined as the number of timesthat t occurs in d .Frequency in document vs. frequency in collection• In addition, to term frequency (the frequency of the term in the document) . . .• . . . we also want to use the frequency of the term in the collection for weightingand ranking.• Rare terms are more informative than frequent terms.• Consider a term in the query that is rare in the collection (e.g., ARACHNOCENTRIC).• A document containing this term is very likely to be relevant.• → We want high weights for rare terms like ARACHNOCENTRIC.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.42Desired weight for frequent termsWeighting scheme• Frequent terms are less informative than rare terms.• Consider a term in the query that is frequent in the collection (e.g., GOOD,INCREASE, LINE).• A document containing this term is more likely to be relevant than a documentthat doesn’t . . .• . . . but words like GOOD, INCREASE and LINE are not sure indicators ofrelevance.• → For frequent terms like GOOD, INCREASE, and LINE, we want positiveweights . . .• . . . but lower weights than for rare terms.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.43Document FrequencyDocument Frequency (df)• We want high weights for rare terms like ARACHNOCENTRIC.• We want low (positive) weights for frequent words like GOOD, INCREASE, andLINE.• We will use document frequency to factor this into computing the matchingscore.• The document frequency is the number of documents in the collection that theterm occurs in.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.44idf weightinverse document frequency (idf)• dft is the document frequency, the number of documents that t occurs in.• dft is an inverse measure of the informativeness of term t .• We define the idf weight of term t as follows:idft = log10Ndft(N is the number of documents in the collection.)• idft is a measure of the informativeness of the term.• [logN/dft ] instead of [N/dft ] to “dampen” the effect of idf• Note that we use the log transformation for both term frequency and documentfrequency.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.45Examples for idfCompute idft using the formula: idft = log101,000,000dftterm dft idftcalpurnia 1 6animal 100 4sunday 1000 3fly 10,000 2under 100,000 1the 1,000,000 0Effect of idf on ranking• idf affects the ranking of documents for queries with at least two terms.• For example, in the query “arachnocentric line”, idf weighting increases therelative weight of ARACHNOCENTRIC and decreases the relative weight of LINE.• idf has little effect on ranking for one-term queries.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.46tf-idf weightingComputing tf-idf• The tf-idf weight of a term is the product of its tf weight and its idf weight:wt,d = (1 + log tft,d ) · logNdft• Set to 0 if tft,d = 0• Best known weighting scheme in information retrieval• Note: the “-” in tf-idf is a hyphen, not a minus sign!• Alternative names: tf.idf, tf x idfRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.47Summary: tf-idfComputationAssign a tf-idf weight for each term t in each document d :wt,d ={(1 + log tft,d ) · log Ndft, if tft,d > 00, otherwiseEffectThe tf-idf weight . . .• . . . increases with the number of occurrences within a document(due to the term frequency)• . . . increases with the rarity of the term in the collection(due to the inverse document frequency)→ Worksheet #5: Task 9René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.48Binary incidence matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 1 1 0 0 0 1BRUTUS 1 1 0 1 0 0CAESAR 1 1 0 1 1 1CALPURNIA 0 1 0 0 0 0CLEOPATRA 1 0 0 0 0 0MERCY 1 0 1 1 1 1WORSER 1 0 1 1 1 0. . .Each document is represented as a binary vector ∈ {0,1}|V |.[from Introduction to Information Retrieval]René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.49Count matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 157 73 0 0 0 1BRUTUS 4 157 0 2 0 0CAESAR 232 227 0 2 1 0CALPURNIA 0 10 0 0 0 0CLEOPATRA 57 0 0 0 0 0MERCY 2 0 3 8 5 8WORSER 2 0 1 1 1 5. . .Each document is now represented as a count vector ∈ N|V |.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.50Binary→ count→ weight matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 5.25 3.18 0.0 0.0 0.0 0.35BRUTUS 1.21 6.10 0.0 1.0 0.0 0.0CAESAR 8.59 2.54 0.0 1.51 0.25 0.0CALPURNIA 0.0 1.54 0.0 0.0 0.0 0.0CLEOPATRA 2.85 0.0 0.0 0.0 0.0 0.0MERCY 1.51 0.0 1.90 0.12 5.25 0.88WORSER 1.37 0.0 0.11 4.15 0.25 1.95. . .Each document is now represented as a real-valued vector of tf-idf weights ∈ R|V |.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.51Vector Space ModelDocuments as vectors• Each document is now represented as a real-valued vector of tf-idf weights∈ R|V |.• So we have a |V |-dimensional real-valued vector space.• Terms are axes of the space.• Documents are points or vectors in this space.• Very high-dimensional: tens of millions of dimensions when you apply this toweb search engines• Each vector is very sparse – most entries are zero.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.52Queries as vectors• Key idea 1: do the same for queries: represent them as vectors in thehigh-dimensional space• Key idea 2: Rank documents according to their proximity to the query• proximity = similarity• proximity ≈ negative distance• Recall: We’re doing this because we want to get away from theyou’re-either-in-or-out, feast-or-famine Boolean model.• Instead: rank relevant documents higher than nonrelevant documentsRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.53Cosine similarity between query and documentcos(~q, ~d) = SIM(~q, ~d) =~q · ~d|~q||~d |=∑|V |i=1 qidi√∑|V |i=1 q2i√∑|V |i=1 d2i• qi is the tf-idf weight of term i in the query.• di is the tf-idf weight of term i in the document.• |~q| and |~d | are the lengths of ~q and ~d .• This is the cosine similarity of ~q and ~d . . . . . . or, equivalently, the cosine of theangle between ~q and ~d .René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.54Cosine similarity illustrated0 101richpoor~v(q)~v(d1)~v(d2)~v(d3)θ.René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.55Basic Recommender Engine using Vector Space ModelApproach• Represent all documents (movie descriptions, blog posts, researcharticles, . . . ) as a weighted tf-idf vector• Compute the cosine similarity between the target vector and each documentvector• Rank documents with respect to the target• Return the top k (e.g., k = 10) to the userRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.56SummaryVector Space Model• A mathematical model to portray an n-dimensional space• Entities are described by vectors with n coordinates in a real space Rn• Given two vectors, we can compute a similarity coefficient between them• Cosine of the angle between two vectors reflects their degree of similaritytf = 1 + log(tft,d ) (1)idf = logNdft(2)cos(~q , ~d ) =∑|v|i=1 qi · di√∑|v|i=1 q i2 ·√∑|v|i=1 d i2(3)René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.58Outline1 Introduction2 Collaborative Filtering3 Content-based Recommendations4 Notes and Further ReadingRené WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.59Reading MaterialRequired• [Ala09, Chapters 2, 3] (Recommendations)• [MRS08, Chapter 8] (Evaluation)Supplemental• [MRS08, Chapter 6] (Vector Space Model, tf-idf)René WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.60References[Ala09] Satnam Alag.Collective Intelligence in Action.Manning, 2009.https://concordiauniversity.on.worldcat.org/oclc/314121652.[MRS08] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze.Introduction to Information Retrieval.Cambridge University Press, 2008.http://informationretrieval.org.[TB16] Doug Turnbull and John Berryman.Relevant Search.Manning, 2016.https://concordiauniversity.on.worldcat.org/oclc/954339855.https://concordiauniversity.on.worldcat.org/oclc/314121652http://informationretrieval.orghttps://concordiauniversity.on.worldcat.org/oclc/954339855	Recommender Systems	Introduction	Modeling Users	Collaborative Filtering	Introduction	Computing with Words	Item Recommendation	Items Related to other Items	Items of Interest to a User	Relevant Users for an Item	Semantic User Profiles	Evaluation	Content-based Recommendations	Motivation	TF*IDF weighting	Term Vector Space Model	Summary	Notes and Further Reading