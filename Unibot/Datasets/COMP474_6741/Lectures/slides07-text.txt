René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.1Lecture 7Machine Learning for Intelligent SystemsIntroduction, Clustering, Classification, Regression, EvaluationCOMP 474/6741, Winter 2022René WitteDepartment of Computer Scienceand Software EngineeringConcordia UniversityRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.2Outline1 Machine Learning PrimerHistoryML TypesProcess2 Clustering DocumentsMotivationk-Means ClusteringApplication Example3 Classifications & PredictionsIntroductionClassification with kNNRegression with kNN4 Machine Learning EvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-Validation5 Notes and Further ReadingRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.3AI, ML, DLhttps://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49bhttps://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49bRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.4HistoryLearn from experienceIn 1959, Arthur Samuel first proposed the conceptMachine Learning:“A computer program is said to learn fromexperience E with respect to some class of tasksT and performance measure P if its performanceat tasks in T, as measured by P, improves withexperience E.”René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.5Automated ReasoningInferenceProcess of deriving new facts from a set of premisesTypes of logical inference1 Deduction2 Abduction3 InductionRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.6Deductionaka Natural Deduction• Conclusion follows necessary from the premises.• From A⇒ B and A, we conclude that B• We conclude from the general case to a specific example of the general case• Example:1 All men are mortal.2 Socrates is a man.3 from 1 ∧ 2 ⇒ Socrates is mortal.• Our subclass inference in RDFS also falls into this category.René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.7AbductionAbductive Reasoning• Conclusion is one hypothetical (most probable) explanation for the premises• From A⇒ B and B, we conclude A• Example:1 Drunk people do not walk straight.2 John does not walk straight.3 from 1 ∧ 2 ⇒ John is drunk.• Not sound. . . but may be most likely explanation for B• Used in medicine. . .1 in reality: disease⇒ symptoms2 patient complains about some symptoms. . . doctor concludes a diseaseRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.8InductionInductive Reasoning• Conclusion about all members of a class from the examination of only a fewmember of the class.• From A∧C⇒ B and A∧D⇒ B, we conclude A⇒B• We construct a general explanation based on specific cases• Example:1 All CS students in COMP 474 are smart.2 All CS students on vacation are smart.3 from 1 ∧ 2 ⇒ All CS students are smart.• Not sound• But, can be seen as hypothesis construction or generalisationRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.9Inductive LearningLearning from examples• Most work in ML• Examples are given (positive and/or negative) to train a system in aclassification (or regression) task• Extrapolate from the training set to make accurate predictions about futureexamples• Given a new instance X you have never seen, you must find an estimate of thefunction f(X) where f(X) is the desired outputFrom datascience.com, https://towardsdatascience.com/cat- dog- or- elon- musk- 145658489730https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.10Example• Given pairs (X , f (X )) (the training set – the data points)• Find a function f that fits the training set well• So that given a new X , you can predict its f (X ) valueNote: choosing one function over another beyond just looking at the training set iscalled inductive bias (eg. prefer “smoother” functions)René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.11Inductive Learning FrameworkFeature Vectors• Input data are represented by a vector of features, X• Each vector X is a list of (attribute, value) pairs.• Ex: X =[nose:big, teeth:big, eyes:big, moustache:no]• The number of attributes is fixed (positive, finite)• Each attribute has a fixed, finite number of possible values• Each example can be interpreted as a point in a n-dimensional feature space,where n is the number of attributes (features)René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.12Some Machine Learning TechniquesProbabilistic Methods• e.g., Naïve Bayes ClassifierDecision Trees• Use only discriminating features as questions in a big if-then-else treeNeural Networks• Also called parallel distributed processing or connectionist systems• Intelligence arise from having a large number of simple computational unitsNB: Deep Learning ≈ Neural Networks “on steroids”René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.13Supervised LearningLabeled DataIn Supervised Learning, we train a system using data with known labels.René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.14Unsupervised LearningUnlabeled DataIn Unsupervised Learning, we have only unlabeled data and train a system withoutguidance from an expected output.René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.15Reinforcement LearningRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.16AI Learns to Parkhttps://www.youtube.com/watch?v=VMp6pq6_QjIhttps://www.youtube.com/watch?v=VMp6pq6_QjIRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.17Machine Learning CategoriesRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.18http://www.cognub.com/index.php/cognitive-platform/http://www.cognub.com/index.php/cognitive-platform/René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.19General machine learning process→ Worksheet #6: Task 1René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.20Outline1 Machine Learning Primer2 Clustering DocumentsMotivationk-Means ClusteringApplication Example3 Classifications & Predictions4 Machine Learning Evaluation5 Notes and Further ReadingRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.21Math with WordsVector Space Model• A mathematical model to portray an n-dimensional space• Entities are described by vectors with n coordinates in a real space Rn• Given two vectors, we can compute a similarity coefficient between them• Cosine of the angle between two vectors reflects their degree of similaritytf = 1 + log(tft,d ) (1)idf = logNdft(2)cos(~q , ~d ) =∑|v|i=1 qi · di√∑|v|i=1 q i2 ·√∑|v|i=1 d i2(3)René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.22MotivationIntelligent Systems for Investigative JournalismOrganize large, unstructured document collections:• Enron email dataset – ca. 500,000 emails from management• Wikileaks – often releases millions of documents• Guantanamo Bay Files, TPP Agreements, CIA Documents, German BND-NSAInquiry, . . .• Facebook internal documents leaks (Cambridge Analytica scandal, 7000documents)• Luanda Leaks (715,000 emails, charts, contracts, audits, etc.)• Paradise Papers (13.4 million confidential papers regarding offshoreinvestments)René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.23https://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.htmlhttps://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.htmlhttps://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.htmlRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.24https://opensemanticsearch.orghttps://opensemanticsearch.orgRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.25ClusteringUnsupervised Learning• Remember, we do not “classify” documents (like in “spam vs. ham”)• Rather, we group similar documents together• Often used as a first exploratory step in data analysis• Data points (here: documents) in individual clusters can be further analyzed,possibly with different methodsRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.26What are Clusters?Clustering• The organization of unlabeled data into similarity groups, called clusters• A cluster is a collection of data items which are “similar” between them, and“dissimilar” to data items in other clusters.• Generally, there is no right or wrong answer to what the clusters in a datasetare.René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.27Clustering TechniquesRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.28k-Means ClusteringPartition-based ClusteringK-means (MacQueen, 1967) is a partitional clustering algorithm:• Given m vectors in an n-dimensional space, ~x1, . . . , ~xm ∈ Rn• User defines k , the number of clustersAlgorithm1 Pick k points from the dataset (usually at random).These points represent our initial group centroïds.2 Assign each data point ~xi to the nearest centroïd.3 When all data points have been assigned, recalculate the positions of the kcentroïds as the average of the cluster.4 Repeat Steps 2 and 3 until none of the data instances change group(or changes stay below a given convergence limit ∆).René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.29Euclidian DistanceTo find the nearest centroïd:• a possible metric is theEuclidean distance• distance d between 2 points p, qp = (p1,p2, . . . ,pn)q = (q1,q2, . . . ,qn)d =√√√√ n∑i=1(pi − qi )2• where to assign a data point ~x?• → for all k clusters, choose the onewhere ~x has the smallest distanceRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.30Example (1/5)2D-vectors, k=3: Initialize random centroïdsRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.31Example (2/5)Partition data points to closest centroïdsRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.32Example (3/5)Compute new centroïdsRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.33Example (4/5)Re-assign data points to closest new centroïdsRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.34Example (5/5)Repeat until clusters stabilizeRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.35k-Means Clustering Illustratedhttps://www.youtube.com/watch?v=5I3Ei69I40s→ Worksheet #6: Task 2https://www.youtube.com/watch?v=5I3Ei69I40sRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.36k-Means: Pros & ConsPros• Simple, easy to understand and implement• Converges very fast• Efficient: Time complexity O(t · k · n), with• n number of data points• k number of clusters• t number of iterations→ considered linear for practical purposesCons• User needs to choose k (usually not known)• Sensitive to outliers• Different results on same dataset, based on initial (random) centroïdsRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.37k-Means & OutliersRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.38k-Means: Sensitivity to Initial SeedsRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.39k-MeansSummary• Despite weaknesses, k-means is still one of the most popular algorithms, dueto its simplicity and efficiency• No clear evidence that any other clustering algorithm performs better in general• Comparing different clustering algorithms is a difficult task:No one knows the correct clusters!René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.40Document Clustering Example: Analyzing NSF Research Grantshttps://www.youtube.com/watch?v=85fZcK5EpnAhttps://www.youtube.com/watch?v=85fZcK5EpnARené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.41Outline1 Machine Learning Primer2 Clustering Documents3 Classifications & PredictionsIntroductionClassification with kNNRegression with kNN4 Machine Learning Evaluation5 Notes and Further ReadingRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.42Classification of DataRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.43Classification AlgorithmsRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.44k-Nearest-Neighbor (kNN) ClassificationkNN AlgorithmTraining: only store feature vectors + class labelsTesting: Find the k data points nearest (e.g., Euclidian distance) to the newvalue. Resulting class is decided by majority vote.Note: in this simple form, kNN has no training effort, but large testing effort(so-called lazy learning)Copyright Antti Ajanki (https://commons.wikimedia.org/wiki/File:KnnClassification.svg), “KnnClassification”,licensed under https://creativecommons.org/licenses/by- sa/3.0/legalcodehttps://commons.wikimedia.org/wiki/File:KnnClassification.svghttps://creativecommons.org/licenses/by-sa/3.0/legalcodeRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.45kNN ClassificationWith k = 1• Compute the distance of the unknown sample to all existing samples• Assign the class of the closest neighbor to the new sample• Distance can be computed with different metrics, e.g.,Euclidean distance or Manhattan distanceCopyright 2017 by O’Reilly Media, Inc., [MG17]René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.46kNN Classification: General caseWith arbitrary k• kNN classification becomes a voting algorithm• assign the same class as the majority of the k closest neighbors to the newsample• Choice of k is dependent on data setCopyright 2017 by O’Reilly Media, Inc., [MG17]René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.47Netflix: Predict Success of Original ContentIn 2013, Netflix decided to commission two seasons of the U.S. remake of theBritish series House of Cards based on an analysis of its customers’ datahttps://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/→ Worksheet #6: Task 3https://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/https://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.48RegressionForecasting or predicting a value: e.g., house price, movie rating, temperature at noon, ...http://www.cognub.com/index.php/cognitive-platform/http://www.cognub.com/index.php/cognitive-platform/René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.49kNN RegressionWith k = 1• Find the nearest existing data point to a new sample as before• Assign the value of this point (e.g., price, rating, ...) to the new instance• Note: given n-dimensional vectors, we are using n − 1 dimensions for the similarityand the final for the predicted valueCopyright 2017 by O’Reilly Media, Inc., [MG17]René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.50kNN Regression: General CaseFind the k nearest existing data pointsAssign the average of their values to the new point• Note that this algorithm cannot extrapolateCopyright 2017 by O’Reilly Media, Inc., [MG17]→ Worksheet #6: Task 4René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.51Machine Learning at Netflixhttps://www.youtube.com/watch?v=X9ZES-fsxgUhttps://www.youtube.com/watch?v=X9ZES-fsxgURené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.52Outline1 Machine Learning Primer2 Clustering Documents3 Classifications & Predictions4 Machine Learning EvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-Validation5 Notes and Further ReadingRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.53Evaluation of a ML ModelMethodology• How do you know if what you learned is correct?• You run your classifier on a data set of unseen examples (that you did not usefor training) for which you know the correct classification (“gold standard”)Training vs. testing data• Split data into training (80%) and testing (20%) sets• Depending on ML algorithm, the training set can be further split into:• Actual training set (80%)• Validation set (20%)René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.54Standard Methodology1 Collect a large set of examples (all with correct classifications)2 Divide collection into training, validation and test set3 Apply learning algorithm to training set to learn the parameters4 Measure performance with the validation set, and adjust hyper-parameters toimprove performance5 Performance not good enough? ⇒ 36 Measure performance with the test setDO NOT LOOK AT THE TEST SETuntil you arrived at Step 6.ParametersBasic values learned by the MLmodel, e.g.:• for NB: prior & conditionalprobabilities• for DTs: features to split• for ANNs: weightsHyper-ParametersParameters used to set up the MLmodel, e.g.:• for NB: value of delta forsmoothing• for DTs: pruning level• for ANNs: # of hidden layers, # ofnodes per layer. . .René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.55MetricsAccuracy• % of instances of the test set the algorithm correctly classifies• when all classes are equally important and representedRecall & Precision• when one class is more important than the othersF-Measure• Combined Precision & Recall (harmonic mean)René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.56ML EvaluationEvaluation of ClassifiersWhat kind of errors can we make?Reality says. . .Positive NegativeModel predicts. . .Positive True Positive (TP) False Positive (FP)Negative False Negative (FN) True Negative (TN)This is a so-called (binary) confusion matrixError Types• False positive classification: Type I error(“convict the innocent!”)• False negative classification: Type II error(“free the guilty!”)Important realization: not all errors are created equal!Voltaire: “It is better to risk saving a guilty man than to condemn an innocent one.”René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.57Evaluation MetricsCommonly used• Accuracy = (TP + TN)/(TP + TN + FP + FN)• Recall = TP/(TP + FN)• Precision = TP/(TP + FP)• F1-score = 2·Precision·RecallPrecision+Recall (harmonic mean)Mind the evaluation taskPrecision, recall etc. are defined slightly differentlyfor:• Information retrieval tasks• Classification tasks• Ranked retrieval tasks• Information extraction tasks→ Worksheet #6: Task 5Copyright by Walber (https://commons.wikimedia.org/wiki/File:Precisionrecall.svg), licensed under the CreativeCommons Attribution-Share Alike 4.0 International licensehttps://creativecommons.org/licenses/by- sa/4.0/legalcode→ Worksheet #6: Task 5https://commons.wikimedia.org/wiki/File:Precisionrecall.svghttps://creativecommons.org/licenses/by-sa/4.0/legalcodeRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.58Confusion Matrix• Where did the learner go wrong ?• Use a confusion matrix (contingency table)René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.59Learning CurveCopyright 2007–2019, scikit-learn developers (BSD License), https://scikit- learn.org/stable/auto_examples/model_selection/plot_learning_curve.htmlPlot evaluation metric vs. size of training set• the more, the better• but after a while, not much improvement. . .https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.htmlRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.60Some Words on Training. . .Watch out for:• Noisy Data• Overfitting/UnderfittingRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.61Noisy DataCommon issues• Two examples have the same feature-value pairs, but different outputs• Some values of features are incorrect or missing (ex. errors in the dataacquisition)• Some relevant attributes are not taken into account in the data setRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.62Overfitting• If a large number of irrelevantfeatures are there, we may findmeaningless regularities in the datathat are particular to the training databut irrelevant to the problem.• Complicated boundaries overfit thedata (a.k.a. overtraining)• they are too tuned to the particulartraining data at hand• They do not generalize well to thenew data• Extreme case: “rote learning”• Training error is low• Testing error is highCopyright by Chabacano (https://commons.wikimedia.org/wiki/File:Overfitting.svg) license under the Creative Commons Attribution-ShareAlike 4.0 International license, https://creativecommons.org/licenses/by- sa/4.0/legalcodehttps://commons.wikimedia.org/wiki/File:Overfitting.svghttps://creativecommons.org/licenses/by-sa/4.0/legalcodeRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.63Underfitting• We can also underfit data, i.e. usetoo simple decision boundary• Model is not expressive enough (notenough features)• a.k.a. Undertraining• There is no way to fit a lineardecision boundary so that thetraining examples are well separated• Training error is high• Testing error is highRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.64Example: Animal ClassificationFeaturesWhat about cat vs. dog?[from: Alison Cawsey: The Essence of AI (1997)]René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.65Cross-ValidationData Scarcity• there is never enough training data• so testing data is precious as wellk-fold Cross-Validation‘Re-use’ different parts of the training data for testing. E.g., 10-fold cross-validation:• split data into 10 equal parts• train on 9 of these, test on the 10th• repeat 10 times, resulting in 10 different performance results• average these for overall performanceRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.66Outline1 Machine Learning Primer2 Clustering Documents3 Classifications & Predictions4 Machine Learning Evaluation5 Notes and Further ReadingRené WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.67Reading MaterialRequired• [MG17, Chapters 2, 3, 5] (kNN, k-Means, Evaluation)Supplemental• [PS12, Chapter 7] (ML Training)• [PS12, Chapter 8] (Testing and Evaluation)René WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.68References[MG17] Andreas C Müller and Sarah Guido.Introduction to Machine Learning with Python.O’Reilly, 2017.https://concordiauniversity.on.worldcat.org/oclc/960211579.[PS12] James Pustejovsky and Amber Stubbs.Natural Language Annotation for Machine Learning.O’Reilly, 2012.https://concordiauniversity.on.worldcat.org/oclc/801812987.https://concordiauniversity.on.worldcat.org/oclc/960211579https://concordiauniversity.on.worldcat.org/oclc/801812987	Intro to ML	Machine Learning Primer	History	ML Types	Process	Clustering Documents	Motivation	k-Means Clustering	Application Example	Classifications & Predictions	Introduction	Classification with kNN	Regression with kNN	Machine Learning Evaluation	Evaluation Methodology	Evaluation Metrics	Error Analysis	Overfitting	Underfitting	Cross-Validation	Notes and Further Reading